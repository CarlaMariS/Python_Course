{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://communications.univie.ac.at/fileadmin/_processed_/csm_Uni_Logo_2016_2f47aacf37.jpg\" \n",
    "     alt=\"Logo Universität Wien\" \n",
    "     width=\"200\"/>\n",
    "\n",
    "# Practical Machine Learning for Natural Language Processing - 2023 SS  \n",
    "\n",
    "### Assigment 1 - Python for Poets  \n",
    "\n",
    "This assigment is an adaptation for Python of the original exercise [\"Unix for Poets\"](https://www.cs.upc.edu/~padro/Unixforpoets.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBR said Friday the global economic downturn so far has\n",
      "had\n",
      "little effect on its business but warned some projects on its books\n",
      "could be in jeopardy if the headwinds persist into next year.\n",
      "\n",
      "\"With the economic outlook remaining uncertain, it is possible\n",
      "that\n",
      "customers may cancel or delay projects that are under way,\" said\n",
      "William\n",
      "Utt, chief executive of the Houston-based engineering and\n",
      "construction\n",
      "giant and government contractor.\n",
      "\n",
      "He did not predict how much of the company's $15.3billion in\n",
      "fu\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../Data/txt/nyt_200811.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will solve the following exercises using **Pure Python**  \n",
    "### (only packages \"string\" and \"re\" are allowed).  \n",
    "\n",
    "1. Count words in a text  \n",
    "2. Sort a list of words in various ways  \n",
    "   • ascii order   \n",
    "   • \"rhyming\" order   \n",
    "3. Extract useful info for a dictionary  \n",
    "4. Compute ngram statistics  \n",
    "5. Make a Concordance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Count words in a text\n",
    "\n",
    "a. Output a list of words in the file along with their frequency counts (ignoring case).   \n",
    "b. Count how many unique words there are (ignoring case).    \n",
    "c. Check how common are all different sequences of vowels (e.g. the sequences \"ieu\" or just \"e\" in \"lieutenant\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering and formatting the text for further analysis\n",
    "\n",
    "my_text = re.sub(\"[^A-z'\\n]\",\" \", text) #filtering the text, so it only includes letters, tabs and apostrophies (therefore excluding numbers etc.)\n",
    "my_text = re.sub(' +',' ', my_text) #substituting multiple blank spaces that appear right after another with only one blank space fur better reading\n",
    "my_text = re.sub(\"\\b[U]{1}\\.[S]{1}\\.\", \"United States\", my_text)\n",
    "my_list = [token.lower() for token in my_text.split()] #splitting the string of the filtered text into a list of all words\n",
    "#print(my_list[0:500])\n",
    "#print(my_text[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) my_count will add all elements e in my_list into a dictionary where my_count{t:f} == {element:frequency}\n",
    "my_count = {}\n",
    "for e in my_list:\n",
    "    if e in my_count:\n",
    "        my_count[e] += 1\n",
    "    else:\n",
    "        my_count[e] = 1\n",
    "#my_count \n",
    "\n",
    "#in the dictionary my_count all words with their frequency are stored, the following step creates a list out of the dictionary\n",
    "\n",
    "my_count_list = list(my_count.items())\n",
    "#print(my_count_list[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28994\n",
      "28994\n"
     ]
    }
   ],
   "source": [
    "# b) Counting the unique values either from the dictionary my_count or from the list my_count_list\n",
    "unique = list(dict.fromkeys(my_count))\n",
    "print(len(unique))\n",
    "print(len(my_count_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) finding all vowels and then adding them to a dictionary with their frequencies vowels_count sorted by frequency\n",
    "\n",
    "v_text = my_text.lower() #the text is made all lowercase to identify same sequences in upper or lower case\n",
    "only_vowels = re.findall(\"[aeiouyAEIOUY]+\", v_text)\n",
    "\n",
    "vowels_count = {}\n",
    "for v in only_vowels:\n",
    "    if v in vowels_count:\n",
    "        vowels_count[v] += 1\n",
    "    else:\n",
    "        vowels_count[v] = 1\n",
    "vowels_count = sorted(vowels_count.items(), key=lambda x:x[1], reverse=True)\n",
    "#print(vowels_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Sorting and reversing lines of text\n",
    "\n",
    "a. Sort each line alphabetically (ignoring case).  \n",
    "b. Sort in numeric ([ascii](https://python-reference.readthedocs.io/en/latest/docs/str/ASCII.html)) order.  \n",
    "c. Alphabetically reverse sort (ignoring case).  \n",
    "d. Sort in reverse numeric ([ascii](https://python-reference.readthedocs.io/en/latest/docs/str/ASCII.html)) order.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) after transforming the filtered text to lowercase, it is split by lines and the words in each line are sorted alphabetically in the list a_sort\n",
    "a_sort = my_text.lower().split(\"\\n\")\n",
    "a_sort = [item.split() for item in a_sort]\n",
    "#[item.sort() for item in a_sort] #doesnt return anything because immutable -> call copy!!! for those issues\n",
    "#a_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) the original text with all its characters is used for an accurate result and split by lines then sorted by the ascii values into the list n_sort\n",
    "numeric = text.split(\"\\n\")\n",
    "#print(numeric[0:100])\n",
    "n_sort = []\n",
    "for element in numeric:\n",
    "    n_sort= sorted(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) copying the alphabetically sorted list and reversing the sort into the list a_reverse\n",
    "\n",
    "a_reverse = a_sort.copy()\n",
    "#[item.sort(reverse=True) for item in a_reverse]\n",
    "#a_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d) again, means letter by letter\n",
    "#numeric = text.split(\"\\n\")\n",
    "#print(numeric[0:100])\n",
    "n_reverse = n_sort.copy()\n",
    "print(n_sort)\n",
    "sorted(n_reverse, reverse=True)\n",
    "\n",
    "#for element in numeric:\n",
    "    #n_reverse= sorted(element, reverse=True)\n",
    "    #print(n_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Computing basic statistics\n",
    "\n",
    "a. Find the 50 most common words  \n",
    "b. Find the words in the NYT that end in \"zz\"  \n",
    "c. Count the lines, the words, and the characters  \n",
    "d. How many all uppercase words are there in this NYT file?  \n",
    "e, How many 4-letter words?  \n",
    "f. How many different words are there with no vowels?  \n",
    "g. **tricky:** How many “1 syllable” words are there?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\carla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "[('said', 3813), ('one', 1368), ('new', 1202), ('would', 1187), ('obama', 1013), ('like', 964), ('year', 921), ('time', 874), ('two', 874), ('last', 871), ('people', 823), ('first', 815), ('could', 792), ('mccain', 751), ('years', 724), ('also', 692), ('many', 649), ('state', 625), ('even', 578), ('campaign', 578), ('percent', 576), ('president', 562), ('election', 559), ('get', 543), ('three', 527), ('back', 512), ('day', 501), ('going', 482), ('still', 460), ('million', 459), ('game', 457), ('made', 443), ('states', 435), ('much', 433), ('say', 424), ('news', 421), ('way', 418), ('american', 418), ('since', 413), ('government', 407), ('week', 399), ('long', 398), ('think', 395), ('make', 391), ('home', 387), ('go', 382), ('says', 380), ('times', 374), ('may', 371), ('john', 368)]\n",
      "[('the', 29895), ('a', 13486), ('to', 12915), ('of', 12585), ('and', 12408), ('in', 11250), ('that', 5925), ('for', 5156), ('on', 4298), ('is', 4297), ('he', 3947), ('said', 3813), ('with', 3513), ('was', 3466), ('it', 3312), ('at', 3087), ('as', 3009), ('his', 2787), ('but', 2563), ('be', 2461), ('have', 2441), ('by', 2364), ('are', 2207), ('has', 2186), ('not', 2097), ('from', 2088), ('i', 2069), ('an', 2044), ('they', 1835), ('who', 1833), ('this', 1831), ('had', 1585), ('we', 1520), ('their', 1494), ('about', 1432), ('or', 1384), ('will', 1383), ('one', 1368), ('more', 1352), ('were', 1287), ('you', 1222), ('new', 1202), ('when', 1193), ('would', 1187), ('she', 1162), ('been', 1144), ('if', 1105), ('up', 1098), ('her', 1040), ('all', 1032)]\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_en = nltk.corpus.stopwords.words('english')\n",
    "print(len(stop_en))\n",
    "nostop_list = [l for l in my_list if l not in stop_en]\n",
    "\n",
    "nostop_count = {}\n",
    "for t in nostop_list:\n",
    "    if t in nostop_count:\n",
    "        nostop_count[t] += 1\n",
    "    else:\n",
    "        nostop_count[t] = 1\n",
    "most_common_a = sorted(nostop_count.items(), key=lambda x:x[1], reverse=True)\n",
    "most_common_b = sorted(my_count.items(), key=lambda x:x[1], reverse=True)\n",
    "print(most_common_a[0:50])\n",
    "print(most_common_b[0:50])\n",
    "#print(nostop_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buzz', 'buzz', 'jazz', 'buzz', 'buzz', 'jazz', 'Jazz', 'jazz', 'jazz', 'jazz', 'jazz', 'jazz', 'Buzz', 'Buzz', 'Buzz', 'Jazz', 'buzz', 'Jazz', 'pizazz', 'Buzz', 'buzz', 'Jazz', 'buzz']\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# b) zz words -> re.findall \\w+[z]\\s? or something similar\n",
    "zz_words = re.findall(r\"\\w+[z]{2}\\b\", my_text)\n",
    "print(zz_words)\n",
    "print(len(zz_words))\n",
    "#zz_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3052306\n",
      "506707\n",
      "70335\n"
     ]
    }
   ],
   "source": [
    "# c)\n",
    "print(len(text)) #number of all characters in text\n",
    "print(len(my_list)) #\n",
    "print(len(a_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8117\n"
     ]
    }
   ],
   "source": [
    "# d)\n",
    "upper_case = re.findall(r\"\\b[A-Z]+\\.?[A-Z]*\\b\", text)\n",
    "#print(text[0:1000])\n",
    "#print(upper_case)\n",
    "print(len(upper_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86943\n"
     ]
    }
   ],
   "source": [
    "# e)\n",
    "four_letters = re.findall(r\"\\b[A-z]{4}\\b\", my_text)\n",
    "#print(my_text[0:500])\n",
    "#print(four_letters[0:1000])\n",
    "print(len(four_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor tweet in tweets:\\n    match = re.findall('@\\\\w+', tweet)\\n    if match:\\n        print(match)\\n        \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f)\n",
    "\n",
    "no_vowels = re.findall(r\"^\\b[AEIOUYaeiouy]*\\b\", my_text)\n",
    "print(no_vowels[0:100])\n",
    "\n",
    "\"\"\"\n",
    "for tweet in tweets:\n",
    "    match = re.findall('@\\w+', tweet)\n",
    "    if match:\n",
    "        print(match)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Compute ngrams  \n",
    "\n",
    "a. Find the 10 most common bigrams  \n",
    "b. Find the 10 most common trigrams  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'the'), 3164), (('in', 'the'), 2817), (('to', 'the'), 1213), (('on', 'the'), 1159), (('for', 'the'), 943), (('and', 'the'), 865), (('in', 'a'), 860), (('at', 'the'), 778), (('to', 'be'), 743), (('with', 'the'), 616), (('he', 'said'), 588), (('that', 'the'), 542), (('from', 'the'), 524), (('of', 'a'), 518), (('by', 'the'), 490), (('as', 'a'), 476), (('it', 'was'), 454), (('with', 'a'), 451), (('for', 'a'), 444), (('is', 'a'), 428), (('one', 'of'), 404), (('the', 'first'), 390), (('more', 'than'), 372), (('and', 'a'), 369), (('he', 'was'), 362), (('has', 'been'), 354), (('to', 'a'), 347), (('on', 'a'), 338), (('going', 'to'), 337), (('as', 'the'), 336), (('it', 'is'), 332), (('new', 'york'), 328), (('said', 'the'), 325), (('will', 'be'), 321), (('have', 'been'), 316), (('was', 'a'), 312), (('of', 'his'), 294), (('is', 'the'), 290), (('the', 'new'), 287), (('about', 'the'), 283), (('that', 'he'), 276), (('but', 'the'), 275), (('would', 'be'), 273), (('out', 'of'), 265), (('a', 'lot'), 260), (('at', 'a'), 255), (('according', 'to'), 252), (('said', 'he'), 249), (('in', 'his'), 243), (('have', 'to'), 242)]\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "text_zip = zip(my_list, my_list[1:])\n",
    "bigrams = [item for item in text_zip]\n",
    "#print(bigrams[0:10])\n",
    "\n",
    "freq_bigrams = {}\n",
    "for g in bigrams:\n",
    "    if g in freq_bigrams:\n",
    "        freq_bigrams[g] +=1\n",
    "    else:\n",
    "        freq_bigrams[g] = 1\n",
    "common_bigrams = sorted(freq_bigrams.items(), key=lambda item: item[1], reverse = True)\n",
    "print(common_bigrams[0:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('new', 'york'), 328), (('united', 'states'), 187), (('barack', 'obama'), 183), (('john', 'mccain'), 167), (('last', 'week'), 163), (('year', 'old'), 151), (('last', 'year'), 132), (('years', 'ago'), 121), (('white', 'house'), 87), (('amp', 'amp'), 86), (('first', 'time'), 85), (('sen', 'john'), 81), (('e', 'mail'), 80), (('york', 'times'), 78), (('los', 'angeles'), 77), (('last', 'month'), 76), (('two', 'years'), 75), (('election', 'day'), 75), (('sen', 'barack'), 75), (('high', 'school'), 73), (('said', 'would'), 72), (('vice', 'president'), 67), (('north', 'carolina'), 64), (('sarah', 'palin'), 62), (('private', 'equity'), 60), (('presidential', 'election'), 60), (('four', 'years'), 56), (('next', 'year'), 54), (('tampa', 'bay'), 53), (('super', 'bowl'), 52), (('third', 'quarter'), 50), (('president', 'bush'), 50), (('presidential', 'campaign'), 50), (('web', 'site'), 49), (('early', 'voting'), 49), (('many', 'people'), 49), (('world', 'series'), 49), (('obama', 'campaign'), 48), (('globe', 'com'), 48), (('new', 'hampshire'), 47), (('financial', 'crisis'), 47), (('health', 'care'), 46), (('world', 'war'), 45), (('york', 'city'), 45), (('obama', 'said'), 45), (('news', 'service'), 45), (('san', 'francisco'), 44), (('three', 'years'), 43), (('running', 'back'), 41), (('texas', 'tech'), 41)]\n"
     ]
    }
   ],
   "source": [
    "stopped_text_zip = zip(nostop_list, nostop_list[1:])\n",
    "stopped_bigrams = [item for item in stopped_text_zip]\n",
    "#print(bigrams[0:10])\n",
    "\n",
    "sf_bigrams = {}\n",
    "for g in stopped_bigrams:\n",
    "    if g in sf_bigrams:\n",
    "        sf_bigrams[g] +=1\n",
    "    else:\n",
    "        sf_bigrams[g] = 1\n",
    "stopped_bigrams = sorted(sf_bigrams.items(), key=lambda item: item[1], reverse = True)\n",
    "print(stopped_bigrams[0:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Make a Concordance\n",
    "\n",
    "a. Create a concordance display for an arbitrary word. See the example below  \n",
    "\n",
    "![](../../Data/figs/Sample-concordance-lines-of-actually.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en several Doonesbury strips for next week depicting Barack Obama as the winner of the election.  But for the politicall\n",
      "ut for the politically pugnacious cartoonist, banking on an Obama win a week in advance had zero downside.  \"I never con\n",
      "rudeau wrote in an e-mail to the St. Petersburg Times.  \"If Obama wins, I'm in the flow and commenting on a genuine phen\n",
      "ctory that didn't happen and ticking off some readers.  \"If Obama is elected president in (Doonesbury's) world,\" said Ti\n",
      "ing to Trudeau, who said it would be a \"great idea\" to keep Obama president in Doonesbury even if he loses.  At the St.\n",
      "What would it say about American voters if you are wrong?  Obama would graciously say that the voters have spoken. Some\n",
      "was conceived.  His wife, Tipper, at his side and a Barack Obama banner stretched across the stage, Gore engaged in a s\n",
      "e former vice president was making his first appearance for Obama in Florida, with stops in West Palm Beach and Pompano\n",
      "iday: An overwhelming majority of students voted for Barack Obama, re-elected Gov. Christine Gregoire and U.S. Rep. Jim\n",
      "y afternoon with a majority of students statewide selecting Obama, while challenger Dino Rossi appeared to edge out Greg\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "word = 'Obama'\n",
    "pos = 0 \n",
    "positions = []\n",
    "\n",
    "new_version = re.sub('\\n',' ', text)\n",
    "new_version\n",
    "\n",
    "while pos != -1:\n",
    "    position = new_version.find(word,pos+1)\n",
    "    pos = position\n",
    "    positions.append(position)\n",
    "positions.pop()\n",
    "#print(positions)\n",
    "added_words = 60\n",
    "for position in positions[0:10]:\n",
    "    print(new_version[position - added_words:position + added_words].strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Credit – Secret Message\n",
    "+ The answers to the extra credit exercises will reveal a secret message.  \n",
    "+ We will be working with the following text file for these exercises:  \n",
    "[Link to Text](https://web.stanford.edu/class/cs124/lec/secret_ec.txt)  \n",
    "(No starter code in the Extra Credit)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Credit Exercise 1\n",
    "• Find the 2 most common words in secret_ec.txt containing the letter e.  \n",
    "• Your answer will correspond to the first two words of the secret message.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./secret_text.txt\", \"r\") as f:\n",
    "    secret = f.read()\n",
    "#print(secret)\n",
    "\n",
    "only_e = []\n",
    "secret_list = secret.split()\n",
    "for element in secret_list:\n",
    "    if \"e\" in element:\n",
    "        only_e.append(element)\n",
    "count_e = {}\n",
    "for w in only_e:\n",
    "    if w in count_e:\n",
    "        count_e[w] += 1\n",
    "    else:\n",
    "        count_e[w] = 1\n",
    "most_e = sorted(count_e.items(), key=lambda x:x[1], reverse=True)\n",
    "#most_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Credit Exercise 2\n",
    "• Find the 2 most common bigrams in secret_ec.txt where the second word in the bigram ends with a consonant.  \n",
    "• Your answer will correspond to the next four words of the secret message.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "words_zip = zip(secret_list, secret_list[1:])\n",
    "two_grams_list = [item for item in words_zip]\n",
    "#rint(two_grams_list)\n",
    "\n",
    "count_freq = {}\n",
    "for item in two_grams_list:\n",
    "    if item in count_freq:\n",
    "        count_freq[item] +=1\n",
    "    else:\n",
    "        count_freq[item] = 1\n",
    "secret_bigrams = sorted(count_freq.items(), key=lambda item: item[1], reverse = True)\n",
    "#print(secret_bigrams)\n",
    "print(secret_bigrams[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Credit Exercise 3\n",
    "• Find all 5-letter-long words that only appear once in secret_ec.txt.   \n",
    "• Concatenate your result. This will be the final word of the secret message.  \n",
    "\n",
    "What is the secret message?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every', 'thing']\n"
     ]
    }
   ],
   "source": [
    "five_letters = re.findall(r\"\\b[A-z]{5}\\b\", secret)\n",
    "#print(five_letters)\n",
    "five_counts = {}\n",
    "for w in five_letters:\n",
    "    if w in five_counts:\n",
    "        five_counts[w] += 1\n",
    "    else:\n",
    "        five_counts[w] = 1\n",
    "#print(five_counts)\n",
    "five_result = []\n",
    "for element in five_counts:\n",
    "    c = five_counts[element]\n",
    "    if c == 1:\n",
    "        five_result.append(element)\n",
    "print(five_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best part about unix is every thing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
